{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eZ-7Eh0v7euY",
        "YTZHqwoh8KSf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandossrecife/my_validation3/blob/main/my_analysis_hadoop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup do Ambiente de Análise"
      ],
      "metadata": {
        "id": "3VXzbRhyK7hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf *.log\n",
        "!rm -rf *.txt\n",
        "!rm -rf *.xlsx\n",
        "!rm -rf my_issues\n",
        "!rm -rf hadoop"
      ],
      "metadata": {
        "id": "nPHCWuAukPhs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Install Pydriller.')\n",
        "!pip install pydriller > install_pydriller.log\n",
        "print('Install gitpython.')\n",
        "!pip3 install gitpython > install_gitpython.log\n",
        "print('Install Jira Python lib.')\n",
        "!pip install jira > install_jira_python.log\n",
        "print('Install SQLite in Linux')\n",
        "!sudo apt install -y sqlite3 > install_sqlite.log\n",
        "print('All depenpencies installed!')\n",
        "!cat *.log > install.log\n",
        "print('Details in install.log')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wQ-xzYG4nqN",
        "outputId": "5e6d936c-3e6a-4b36-9e88-0c3571ddda9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Install Pydriller.\n",
            "Install gitpython.\n",
            "Install Jira Python lib.\n",
            "Install SQLite in Linux\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "All depenpencies installed!\n",
            "Details in install.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importa bibliotecas necessárias"
      ],
      "metadata": {
        "id": "eZ-7Eh0v7euY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XUVu8pQm4ifo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "from pydriller import Repository\n",
        "from jira import JIRA\n",
        "import pandas as pd\n",
        "import re\n",
        "import scipy.stats as stats\n",
        "import random\n",
        "import shutil\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variáveis Globais"
      ],
      "metadata": {
        "id": "aln3JNKYAVgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_repository = 'hadoop'\n",
        "url_to_repository = 'https://github.com/apache/hadoop.git'\n",
        "os.environ['MY_REPOSITORY'] = url_to_repository\n",
        "\n",
        "JIRA_SERVER = 'https://issues.apache.org/jira'\n",
        "ISSUE_TRACKER_PROJECT = 'HADOOP'\n",
        "\n",
        "# Credentials\n",
        "os.environ['USERNAME'] = ''\n",
        "os.environ['PASSWORD'] = ''\n",
        "username = os.environ.get('USERNAME')\n",
        "password = os.environ.get('PASSWORD')\n",
        "\n",
        "lista_arquivos_criticos = ['Configuration.java', 'Writable.java', 'StringUtils.java', 'FSDataOutputStream.java', 'BytesWritable.java', 'WritableComparable.java', 'DatanodeProtocol.java', 'ClientProtocol.java', 'FSNamesystem.java', 'DataNode.java','BlockManager.java', 'ResourceScheduler.java', 'ContainerManager.java', 'FairScheduler.java','CapacityScheduler.java', 'NodeManager.java', 'Job.java', 'Mapper.java', 'Reducer.java', 'InputFormat.java', 'OutputFormat.java']"
      ],
      "metadata": {
        "id": "OEbrNLLn5wq5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clona o Repositório"
      ],
      "metadata": {
        "id": "Ti5lAYXOAY1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Clona o repositório : {url_to_repository}')\n",
        "!git clone $MY_REPOSITORY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWfNScS_5uvT",
        "outputId": "4293fe50-bcf5-468d-91b5-485654ab1acb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clona o repositório : https://github.com/apache/hadoop.git\n",
            "Cloning into 'hadoop'...\n",
            "remote: Enumerating objects: 4178088, done.\u001b[K\n",
            "remote: Counting objects: 100% (10274/10274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2642/2642), done.\u001b[K\n",
            "remote: Total 4178088 (delta 8658), reused 8971 (delta 7488), pack-reused 4167814\u001b[K\n",
            "Receiving objects: 100% (4178088/4178088), 2.12 GiB | 20.59 MiB/s, done.\n",
            "Resolving deltas: 100% (3011322/3011322), done.\n",
            "Updating files: 100% (15308/15308), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd hadoop && git log --pretty=\"%H %s\" > all_commits_msg.txt\n",
        "!cd hadoop && git log --pretty=\"%H;%ai;%s\" > all_commits_full.txt\n",
        "\n",
        "# Dados do 1o commit e do ultimo commit\n",
        "!echo \"Último commit: \"\n",
        "!head /content/hadoop/all_commits_full.txt -n 1\n",
        "!echo \"Primeiro commit: \"\n",
        "!tail /content/hadoop/all_commits_full.txt -n 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCPikf0jzDX-",
        "outputId": "1f4777c5-0d68-4fd4-c9b2-6d8fc6dfb526"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Último commit: \n",
            "a32097a921b6a256c82ee8c2a83aa1990c635e0d;2023-11-09 10:14:14 +0100;HADOOP-18954. Filter NaN values from JMX json interface. (#6229).\n",
            "Primeiro commit: \n",
            "5128a9a453d64bfe1ed978cf9ffed27985eeef36;2009-05-19 04:20:40 +0000;HADOOP-4687 Moving src directories on branch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções de apoio"
      ],
      "metadata": {
        "id": "YTZHqwoh8KSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_commits_by_range(initial_date, final_date, repository_name):\n",
        "  \"\"\"Extracts information from a date range of commits\n",
        "  Args:\n",
        "      initial_date: inicial date of commits\n",
        "      final_date: final date of commits\n",
        "  Returns:\n",
        "        A dictionary of commits, where the keys are the commit hashes and the\n",
        "        values are tuples containing the commit message, commit's date, commit's line, commit's file, a list of files modified in the commit, a list of all diffs from modified files\n",
        "  \"\"\"\n",
        "  dict_commit_modified_files = {}\n",
        "  print('Wait...')\n",
        "  my_traverser_commits = Repository(repository_name, since=initial_date, to=final_date).traverse_commits()\n",
        "  total_commits = len(list(my_traverser_commits))\n",
        "  try:\n",
        "    for commit in tqdm.tqdm(Repository(repository_name, since=initial_date, to=final_date).traverse_commits(), total=total_commits, desc=\"Progress commit anlysis\"):\n",
        "      list_of_modified_files = []\n",
        "      list_dict_of_diff_modified_files = []\n",
        "      for m in commit.modified_files:\n",
        "        dict_m_diff = {}\n",
        "        if m is not None:\n",
        "          list_of_modified_files.append(m.filename)\n",
        "          dict_m_diff[m.filename] = m.diff\n",
        "          list_dict_of_diff_modified_files.append(dict_m_diff)\n",
        "      data_commit = str(commit.committer_date.day) + '/' + str(commit.committer_date.month) + '/' + str(commit.committer_date.year)\n",
        "      element = commit.msg, data_commit, commit.lines, commit.files, list_of_modified_files, list_dict_of_diff_modified_files\n",
        "      dict_commit_modified_files[commit.hash] = element\n",
        "  except Exception as ex:\n",
        "    print(f'Erro during travesse commits: {str(ex)}')\n",
        "  return dict_commit_modified_files, total_commits\n",
        "\n",
        "def get_commits_by_range_and_critical_files(initial_date, final_date, critical_files, repository_name):\n",
        "  \"\"\"Extracts information from a date range of commits, focusing on critical files.\n",
        "  Args:\n",
        "      initial_date: inicial date of commits\n",
        "      final_date: final date of commits\n",
        "      critical_files: A list of critical file paths. ex: ['StorageService.java', 'ColumnFamilyStore.java']\n",
        "  Returns:\n",
        "        A dictionary of commits, where the keys are the commit hashes and the\n",
        "        values are tuples containing the commit message, commits's date, commit's line, commit's file, a list of critical files modified in the commit, a list_of_modified_files, a dictionary of diff [filename]:filename.diff, a list of all diffs from modified files\n",
        "        commit.msg, data_commit, commit.lines, commit.files, list_of_critical_files_modified, list_of_modified_files, list_dict_of_diff_files, list_dict_of_diff_modified_files\n",
        "  \"\"\"\n",
        "\n",
        "  dict_commit_modified_files = {}\n",
        "  print('Wait...')\n",
        "  my_traverser_commits = Repository(repository_name, since=initial_date, to=final_date).traverse_commits()\n",
        "  total_commits = len(list(my_traverser_commits))\n",
        "  try:\n",
        "    for commit in tqdm.tqdm(Repository(repository_name, since=initial_date, to=final_date).traverse_commits(), total=total_commits, desc=\"Progress critical files commit anlysis\"):\n",
        "      list_of_critical_files_modified = []\n",
        "      list_of_modified_files = []\n",
        "      list_dict_of_diff_files = []\n",
        "      list_dict_of_diff_modified_files = []\n",
        "      for m in commit.modified_files:\n",
        "        dict_diff_file = {}\n",
        "        list_of_modified_files.append(m.filename)\n",
        "        list_dict_of_diff_modified_files.append(m.diff)\n",
        "        if m.filename in critical_files:\n",
        "          list_of_critical_files_modified.append(m.filename)\n",
        "          dict_diff_file[m.filename] = m.diff\n",
        "          list_dict_of_diff_files.append(dict_diff_file)\n",
        "      if len(list_of_critical_files_modified) > 0:\n",
        "        data_commit = str(commit.committer_date.day) + '/' + str(commit.committer_date.month) + '/' + str(commit.committer_date.year)\n",
        "        element = commit.msg, data_commit, commit.lines, commit.files, list_of_critical_files_modified, list_of_modified_files, list_dict_of_diff_files, list_dict_of_diff_modified_files\n",
        "        dict_commit_modified_files[commit.hash] = element\n",
        "  except Exception as ex:\n",
        "    print(f'Erro during travesse commits: {str(ex)}')\n",
        "  return dict_commit_modified_files, total_commits"
      ],
      "metadata": {
        "id": "blFSkQG57Uwo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JiraIssue:\n",
        "  def __init__(self, key, summary, issue_type, status, priority, description, comments, created_date=None, updated_date=None, resolved_date=None):\n",
        "    self.key = key\n",
        "    self.summary = summary\n",
        "    self.issue_type = issue_type\n",
        "    self.status = status\n",
        "    self.priority = priority\n",
        "    self.description = description\n",
        "    self.comments = comments\n",
        "    self.created_date = created_date\n",
        "    self.updated_date = updated_date\n",
        "    self.resolved_date = resolved_date\n",
        "\n",
        "  def get_comments(self):\n",
        "    return self.comments\n",
        "\n",
        "  def __str__(self):\n",
        "    return (f'Key: {self.key}, Summary: {self.summary}, Type: {self.issue_type}, Status: {self.status}')\n",
        "\n",
        "class JiraIssues:\n",
        "  def __init__(self,project, issues):\n",
        "    self.project = project\n",
        "    self.issues = issues\n",
        "\n",
        "  def add_issue(self, issue):\n",
        "    self.issues.append(issue)\n",
        "\n",
        "  def get_issues(self) -> list:\n",
        "    return self.issues\n",
        "\n",
        "  def update_issues(self, issues):\n",
        "    self.issues = issues\n",
        "\n",
        "  def __str__(self):\n",
        "    str_issues = \"\"\n",
        "    for issue in self.get_issues():\n",
        "      str_issues = str_issues + str(issue)\n",
        "      str_issues = str_issues + ', '\n",
        "    str_issues = '[' + str_issues + ']'\n",
        "    return (f'Project: {self.project}, Qdt of issues: {len(self.issues)}, Issues: {str_issues}')\n",
        "\n",
        "# Classe de utilidades para manipular o servidor Jira\n",
        "class JiraUtils:\n",
        "  def __init__(self, project, jira_instance):\n",
        "    self.project = project\n",
        "    self.jira_jira_instance = jira_instance\n",
        "\n",
        "  def generate_intervals_between_dates(self, date1: tuple, date2: tuple, distance=120) -> list:\n",
        "    start_date = datetime(date1[0], date1[1], date1[2])\n",
        "    end_date = datetime(date2[0], date2[1], date2[2])\n",
        "    interval_days = distance\n",
        "    # Initialize a list to store the intervals\n",
        "    intervals = []\n",
        "    # Initialize the current date as the start date\n",
        "    current_date = start_date\n",
        "    # Loop to generate intervals until the current date is less than or equal to the end date\n",
        "    while current_date < end_date:\n",
        "        interval = (current_date, current_date + timedelta(days=interval_days - 1))\n",
        "        intervals.append(interval)\n",
        "        current_date += timedelta(days=interval_days)\n",
        "    return intervals\n",
        "\n",
        "  def convert_interval_dates(self, dates: list) -> list:\n",
        "    list_interval_dates = []\n",
        "    for each in dates:\n",
        "      date1 = each[0]\n",
        "      # Convert the date to a string in the format \"YYYY/MM/DD\".\n",
        "      str_date1 = date1.strftime(\"%Y/%m/%d\")\n",
        "      date2 = each[1]\n",
        "      str_date2 = date2.strftime(\"%Y/%m/%d\")\n",
        "      elemento = str_date1, str_date2\n",
        "      list_interval_dates.append(elemento)\n",
        "    return list_interval_dates\n",
        "\n",
        "  def generate_list_of_sentences(self, dates: list) -> list:\n",
        "    lista_sentencas = []\n",
        "    for each in dates:\n",
        "      str_date1 = each[0].strftime(\"%Y/%m/%d\")\n",
        "      str_date2 = each[1].strftime(\"%Y/%m/%d\")\n",
        "      sentenca = f'project={self.project.upper()} and created>=\"{str_date1}\" and created<=\"{str_date2}\"'\n",
        "      lista_sentencas.append(sentenca)\n",
        "    return lista_sentencas\n",
        "\n",
        "  def get_list_of_block_issues_by_dates(self,date1, date2, distance=120) -> list:\n",
        "    print('Aguarde...')\n",
        "    t1 = datetime.now()\n",
        "    list_of_dates = self.generate_intervals_between_dates(date1,date2,distance)\n",
        "    lista_sentencas = self.generate_list_of_sentences(list_of_dates)\n",
        "    lista_bloco_issues_by_date = []\n",
        "    total_items = len(lista_sentencas)\n",
        "    i = 0\n",
        "    iterable_lista_sentencas = tqdm.tqdm(lista_sentencas, total=total_items)\n",
        "    for each in iterable_lista_sentencas:\n",
        "      issues_by_date_temp = self.jira_jira_instance.search_issues(each,maxResults=1000)\n",
        "      print(f'Range: {each}, qtd issues: {len(issues_by_date_temp)}')\n",
        "      lista_bloco_issues_by_date.append(issues_by_date_temp)\n",
        "      percentage = (i + 1) / total_items * 100\n",
        "      iterable_lista_sentencas.set_description(f\"Progress Message Analysis\")\n",
        "    i += 1\n",
        "    t2 = datetime.now()\n",
        "    print(t2)\n",
        "    print(f'Tempo da consulta: {t2-t1}')\n",
        "    return lista_bloco_issues_by_date\n",
        "\n",
        "  def concatenate_block_of_issues(self,block_of_issues):\n",
        "    concatenated_list = [item for sublist in block_of_issues for item in sublist]\n",
        "    print(f'Total de issues recuperados: {len(concatenated_list)}')\n",
        "    return concatenated_list\n",
        "\n",
        "def analyze_jira_all_issues(project, all_issues):\n",
        "    \"\"\"Extracts all issues from Issue Tracker\n",
        "        Args:\n",
        "          project: Issue Tracker Project name, example: CASSANDRA\n",
        "          all_issues: a block of issues\n",
        "        Returns:\n",
        "          A list of JiraIssues\n",
        "    \"\"\"\n",
        "    # Create an instance of JiraIssues to manage all issues\n",
        "    my_all_issues = JiraIssues(project, [])\n",
        "    total_items = len(all_issues)\n",
        "\n",
        "    # Iterate through the fetched issues\n",
        "    for issue in tqdm.tqdm(all_issues, total=total_items, desc='Progress jira all issues analysis'):\n",
        "        issue_key = issue.key\n",
        "        issue_summary = issue.fields.summary\n",
        "        issue_description = issue.fields.description\n",
        "        issue_comments = [comment.body for comment in issue.fields.comment.comments]\n",
        "        issue_created_date = issue.fields.created\n",
        "        issue_updated_date = None\n",
        "        issue_resolved_date = issue.fields.resolutiondate\n",
        "\n",
        "        # Check for SATD keywords in the issue's summary, description, and comments\n",
        "        if issue_summary is None:\n",
        "            issue_summary = \"\"\n",
        "        if issue_description is None:\n",
        "            issue_description = \"\"\n",
        "        if issue_comments is None:\n",
        "            issue_comments = \"\"\n",
        "        if issue_created_date == None:\n",
        "            issue_created_date = \"\"\n",
        "        if issue_updated_date == None:\n",
        "            issue_updated_date = \"\"\n",
        "        if issue_resolved_date == None:\n",
        "            issue_resolved_date = \"\"\n",
        "\n",
        "        issue_type = issue.fields.issuetype.name\n",
        "        issue_status = issue.fields.status.name\n",
        "        issue_priority = issue.fields.priority.name\n",
        "\n",
        "        # Create a JiraIssue instance for the SATD issue\n",
        "        my_issue = JiraIssue(issue_key, issue_summary, issue_type, issue_status, issue_priority, issue_description, issue_comments, issue_created_date, issue_updated_date, issue_resolved_date)\n",
        "\n",
        "        my_all_issues.add_issue(my_issue)\n",
        "\n",
        "    return my_all_issues\n",
        "\n",
        "def convert_date_jira_to_datetime(jira_date):\n",
        "  regex = r\"(\\d{4})-(\\d{2})-(\\d{2})\"\n",
        "  match = re.match(regex, jira_date)\n",
        "\n",
        "  if match:\n",
        "      year, month, day = match.groups()\n",
        "      datetime_object = datetime(year=int(year), month=int(month), day=int(day))\n",
        "      return datetime_object\n",
        "  else:\n",
        "      return None\n",
        "\n",
        "def convert_issues_to_dataframe(all_real_issues):\n",
        "  l_issue_key_aux, l_issue_type_aux, l_issue_summary_aux, l_issue_description_aux, l_issue_status_aux, l_issue_priority_aux, l_issue_comments_aux = [], [], [], [], [], [], []\n",
        "  l_issue_created_date, l_issue_resolved_date = [], []\n",
        "\n",
        "  for issue in all_real_issues.get_issues():\n",
        "    l_issue_key_aux.append(issue.key)\n",
        "    l_issue_type_aux.append(issue.issue_type)\n",
        "    l_issue_summary_aux.append(issue.summary)\n",
        "    l_issue_description_aux.append(issue.description)\n",
        "    l_issue_status_aux.append(issue.status)\n",
        "    l_issue_priority_aux.append(issue.priority)\n",
        "    texto_aux = \"\"\n",
        "    for item in issue.get_comments():\n",
        "      texto_aux = texto_aux + str(item) + \"\\n\"\n",
        "    l_issue_comments_aux.append(texto_aux)\n",
        "    created_date_temp = convert_date_jira_to_datetime(issue.created_date)\n",
        "    l_issue_created_date.append(created_date_temp)\n",
        "    resolved_date_temp = convert_date_jira_to_datetime(issue.resolved_date)\n",
        "    l_issue_resolved_date.append(resolved_date_temp)\n",
        "\n",
        "  dict_all_reall_issues_in_commits_detailed = {\n",
        "  'issue_key': l_issue_key_aux,\n",
        "  'issue_type':l_issue_type_aux,\n",
        "  'status':l_issue_status_aux,\n",
        "  'priority':l_issue_priority_aux,\n",
        "  'summary':l_issue_summary_aux,\n",
        "  'description':l_issue_description_aux,\n",
        "  'comments':l_issue_comments_aux,\n",
        "  'created_date': l_issue_created_date,\n",
        "  'resolved_date': l_issue_resolved_date\n",
        "  }\n",
        "\n",
        "  df_all_reall_issues_in_commits_detailed = pd.DataFrame(dict_all_reall_issues_in_commits_detailed)\n",
        "  return df_all_reall_issues_in_commits_detailed"
      ],
      "metadata": {
        "id": "SBj01vqNGR6L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_commits_to_dataframe(dict_of_commits):\n",
        "  '''\n",
        "  v[0] = commit.msg,\n",
        "  v[1] = data_commit,\n",
        "  v[2] = commit.lines,\n",
        "  v[3] = commit.files,\n",
        "  v[4] = list_of_critical_files_modified,\n",
        "  v[5] = list_of_modified_files,\n",
        "  v[6] = list_dict_of_diff_files,\n",
        "  v[7] = list_dict_of_diff_modified_files\n",
        "  '''\n",
        "  l_commit_hash, l_commit_msg, l_commit_data, l_commit_lines, l_commit_files, l_commit_critical_files, l_commit_modified_fies, l_commit_diff_files, l_commit_diff_modified_files = [], [], [], [], [], [], [], [], []\n",
        "  for k, v in dict_of_commits.items():\n",
        "    l_commit_hash.append(k)\n",
        "    l_commit_msg.append(v[0])\n",
        "    l_commit_data.append(v[1])\n",
        "    l_commit_lines.append(v[2])\n",
        "    l_commit_files.append(v[3])\n",
        "    l_commit_critical_files.append(v[4])\n",
        "    l_commit_modified_fies.append(v[5])\n",
        "    l_commit_diff_files.append(v[6])\n",
        "    l_commit_diff_modified_files.append(v[7])\n",
        "\n",
        "  dict_of_commits_aux = {\n",
        "      'hash': l_commit_hash,\n",
        "      'msg': l_commit_msg,\n",
        "      'date': l_commit_data,\n",
        "      'lines': l_commit_lines,\n",
        "      'files': l_commit_files,\n",
        "      'critical_files': l_commit_critical_files,\n",
        "      'modified_files': l_commit_modified_fies,\n",
        "      'diff_files': l_commit_diff_files,\n",
        "      'diff_files_modified_files': l_commit_diff_modified_files\n",
        "  }\n",
        "\n",
        "  df_commits = pd.DataFrame(dict_of_commits_aux)\n",
        "  return df_commits"
      ],
      "metadata": {
        "id": "_VIhIXRaNtVr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_issues_id_by_project(input_string: str, project: str) -> list[str]:\n",
        "    \"\"\"Finds all Cassandra issue ID patterns in the input string.\n",
        "    Args:\n",
        "        input_string: The input string.\n",
        "        project: The pattern related to project name, for example: CASSANDRA project name\n",
        "    Returns:\n",
        "        A list of project issue IDs, if found; otherwise, an empty list.\n",
        "    \"\"\"\n",
        "    # Try to find all Cassandra issue ID patterns in the input string\n",
        "    matches = re.findall(r\"({0}-\\d+)\".format(project), input_string)\n",
        "    # Return an empty list if no matches are found\n",
        "    if not matches:\n",
        "        return []\n",
        "    # Convert the list of matches to a set to remove duplicates\n",
        "    set_matches = set(matches)\n",
        "    # Convert the set of matches back to a list\n",
        "    list_unique_matches = list(set_matches)\n",
        "\n",
        "    # Return the list of matched Cassandra issue IDs\n",
        "    return list_unique_matches\n",
        "\n",
        "def get_commits_with_critical_files_and_issues_in_this_commits(df_commits_with_critical_files):\n",
        "  dict_issues_in_commits = {}\n",
        "  for index in df_commits_with_critical_files.index:\n",
        "    l_issues_in_commit = find_issues_id_by_project(input_string=df_commits_with_critical_files.msg[index], project=ISSUE_TRACKER_PROJECT)\n",
        "    if len(l_issues_in_commit) > 0:\n",
        "      commit_hash = df_commits_with_critical_files.hash[index]\n",
        "      dict_issues_in_commits[commit_hash] = l_issues_in_commit\n",
        "\n",
        "  list_issue_commits, list_issue_issues = [], []\n",
        "  for k, v in dict_issues_in_commits.items():\n",
        "    list_issue_commits.append(k)\n",
        "    for issue in v:\n",
        "      if issue not in list_issue_issues:\n",
        "        list_issue_issues.append(issue)\n",
        "\n",
        "  df_aux  = df_all_reall_issues_in_commits_detailed.copy()\n",
        "  df_issues_in_commits_with_critical_classes = df_aux[df_aux['issue_key'].isin(list_issue_issues)]\n",
        "\n",
        "  return dict_issues_in_commits, df_issues_in_commits_with_critical_classes\n",
        "\n",
        "def calculate_sample_size(confidence_level, margin_of_error, population_proportion, population_size):\n",
        "    # Calculate the Z-score for the given confidence level\n",
        "    z_score = stats.norm.ppf(1 - (1 - confidence_level) / 2)\n",
        "\n",
        "    # Calculate the sample size formula\n",
        "    sample_size = ((z_score**2) * population_proportion * (1 - population_proportion)) / (margin_of_error**2)\n",
        "\n",
        "    # Adjust for finite population\n",
        "    if population_size:\n",
        "        sample_size = sample_size / (1 + ((sample_size - 1) / population_size))\n",
        "\n",
        "    return int(sample_size)"
      ],
      "metadata": {
        "id": "6nD0K5spOxBC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_n_chars(text, max_n):\n",
        "  text_length = len(text)\n",
        "  if text_length <= max_n:\n",
        "    return text\n",
        "  else:\n",
        "    return text[:max_n]\n",
        "\n",
        "def create_new_file(filename, dir_name, issue_type, summary, description, status, comments):\n",
        "  try:\n",
        "    filename = dir_name + '/' + filename\n",
        "\n",
        "    if issue_type is None:\n",
        "      issue_type = ''\n",
        "    if summary is None:\n",
        "      summary = ''\n",
        "    if description is None:\n",
        "      description = ''\n",
        "    if status is None:\n",
        "      status = ''\n",
        "    if comments is None:\n",
        "      comments = ''\n",
        "\n",
        "    with open(filename, mode='w') as f_issue:\n",
        "      f_issue.write(f'issue_type: {issue_type} \\n')\n",
        "      f_issue.write(f'summary: {summary} \\n')\n",
        "      f_issue.write(f'description: {get_max_n_chars(text=description, max_n=1000)} \\n')\n",
        "      f_issue.write(f'status: {status} \\n')\n",
        "      f_issue.write(f'comments: {get_max_n_chars(text=comments, max_n=4000)} \\n')\n",
        "    print(f'File {filename} created with success!')\n",
        "\n",
        "  except Exception as ex:\n",
        "    print(f'Erro ao criar arquivo: {str(ex)}')\n"
      ],
      "metadata": {
        "id": "1wdhaFJxZMYF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleciona randomicamente os issues para inspeção\n",
        "def select_issues_to_inspection(sample_size, df_issues_in_commits_with_critical_classes, my_date):\n",
        "  lista_issues_inspecao = []\n",
        "  dict_issues_para_inspecao = {}\n",
        "  list_issue_key = df_issues_in_commits_with_critical_classes.issue_key.to_list()\n",
        "  list_issue_key = list(set(list_issue_key))\n",
        "  sample_issues = random.choices(list_issue_key, k=sample_size)\n",
        "  dict_issues_para_inspecao[my_date] = sample_issues\n",
        "  print(f'{len(sample_issues)} para inspeção manual')\n",
        "\n",
        "  date_file_name = my_date.split('/')\n",
        "  date_file_name = date_file_name[0] + date_file_name[1] + date_file_name[2]\n",
        "  file_name = 'issues_inspecao_' + date_file_name + '.txt'\n",
        "  with open(file_name, mode='w') as f_temp:\n",
        "    for v in dict_issues_para_inspecao[my_date]:\n",
        "      elemento = v + ','\n",
        "      f_temp.write(elemento)\n",
        "  print(f'Relação de Issues salvos em {my_date} para inspeção.')\n",
        "  return sample_issues\n",
        "\n",
        "# Gera os arquivos .txt de cada issue selecionado para inspeção\n",
        "def generate_files_issues_to_inspection(sample_issues, df_issues_in_commits_with_critical_classes):\n",
        "  contador = 0\n",
        "  my_dir_name = 'my_issues'\n",
        "  if not os.path.exists(my_dir_name):\n",
        "    os.makedirs(my_dir_name)\n",
        "\n",
        "  total_of_issues = df_issues_in_commits_with_critical_classes.shape[0]\n",
        "  for index in tqdm.tqdm(df_issues_in_commits_with_critical_classes.index, total=total_of_issues, desc='Analyzing issues'):\n",
        "    for issue in sample_issues:\n",
        "      if df_issues_in_commits_with_critical_classes.issue_key[index] == issue:\n",
        "        create_new_file(filename=df_issues_in_commits_with_critical_classes.issue_key[index], dir_name=my_dir_name, issue_type=df_issues_in_commits_with_critical_classes.issue_type[index], summary=df_issues_in_commits_with_critical_classes.summary[index], description=df_issues_in_commits_with_critical_classes.description[index], status=df_issues_in_commits_with_critical_classes.status[index], comments=df_issues_in_commits_with_critical_classes.comments[index])\n",
        "        contador += 1\n",
        "  print(f'Foram criados {contador} arquivos para inspeção')"
      ],
      "metadata": {
        "id": "6Tow6aFWiqXK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Extrai commits de um repositório de código"
      ],
      "metadata": {
        "id": "Mu4GQlBW4jTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime(2009, 5, 19, 0, 0, 0)\n",
        "end_date = datetime(2023, 10, 4, 0, 0, 0)\n",
        "extract_all_commits = get_all_commits_by_range(initial_date=start_date, final_date=end_date, repository_name=my_repository)\n",
        "\n",
        "all_commits = extract_all_commits[0]\n",
        "total_all_commits = extract_all_commits[1]\n",
        "\n",
        "print(f'Total de commits extraídos: {total_all_commits}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPY1F6V77MIW",
        "outputId": "57e25af7-ace6-43eb-d925-0f54ae052ba0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wait...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress commit anlysis: 100%|██████████| 26906/26906 [23:18<00:00, 19.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de commits extraídos: 26906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtra os commits com classes críticas"
      ],
      "metadata": {
        "id": "bWPoDyA1Ld38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Analisa a faixa de commits entre: initial_date={str(start_date)}, final_date={str(end_date)}')\n",
        "print('Registra apenas os commits que contem pelo menos um arquivo crítico')\n",
        "filter_commits_with_critical_files = get_commits_by_range_and_critical_files(initial_date=start_date, final_date=end_date, critical_files=lista_arquivos_criticos, repository_name=my_repository)\n",
        "commits_with_critical_files = filter_commits_with_critical_files[0]\n",
        "total_of_commits_with_critical_files = filter_commits_with_critical_files[1]\n",
        "\n",
        "print(f'Total de commits com classes críticas: {total_of_commits_with_critical_files}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsFZ_4-PBMqb",
        "outputId": "ac97b890-11dc-487c-851d-a790da0aea51"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisa a faixa de commits entre: initial_date=2009-05-19 00:00:00, final_date=2023-10-04 00:00:00\n",
            "Registra apenas os commits que contem pelo menos um arquivo crítico\n",
            "Wait...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress critical files commit anlysis: 100%|██████████| 26906/26906 [13:04<00:00, 34.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de commits com classes críticas: 26906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converte os commits com classes críticas para dataframe"
      ],
      "metadata": {
        "id": "bWaZoj8hPxk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_commits_with_critical_files = convert_commits_to_dataframe(dict_of_commits=commits_with_critical_files)\n",
        "df_commits_with_critical_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "8hxQYo8uSXgf",
        "outputId": "e3c9cef1-b35d-4052-85cd-579cd6827dd6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          hash  \\\n",
              "0     5128a9a453d64bfe1ed978cf9ffed27985eeef36   \n",
              "1     bcd64325a11cb0dd5096ffc093d0ffa68c4fcc58   \n",
              "2     b12d765467fd9a4447c473d613d92883fb09c76b   \n",
              "3     a13237975d02b3db913b95845d4b8d6d22f2bac7   \n",
              "4     3200b2ec588dfa5c50f1ec6192ff93ab9187f82d   \n",
              "...                                        ...   \n",
              "2771  d5334fa76170b99f1ddd6b307482d226da12f1a9   \n",
              "2772  4652d22b9195d35eac4d7e02d1f99ebc6a5835c7   \n",
              "2773  ecee022e49269bd6612d32695c6b46a95c6bf11e   \n",
              "2774  26a5f38250de1ac4978427d74c200a968f9a0b65   \n",
              "2775  b87180568b261a617dc9645197615efd4c753786   \n",
              "\n",
              "                                                    msg       date  lines  \\\n",
              "0     HADOOP-4687 Moving src directories on branch\\n...  19/5/2009  67543   \n",
              "1     Merged src/core, src/test/core, src/contrib/ec...  15/6/2009   1799   \n",
              "2     HADOOP-4687. Merge -r 784663:785643 from trunk...  17/6/2009     73   \n",
              "3     HADOOP-2366. Support trimmed strings in Config...  30/6/2009     98   \n",
              "4     HADOOP-6161. Add get/setEnum methods to Config...  20/7/2009     43   \n",
              "...                                                 ...        ...    ...   \n",
              "2771  YARN-6537. Running RM tests against the Router...   3/9/2023    816   \n",
              "2772  HDFS-17178: BootstrapStandby needs to handle R...  12/9/2023    256   \n",
              "2773  HDFS-17197. Show file replication when listing...  25/9/2023     25   \n",
              "2774  HDFS-17204. EC: Reduce unnecessary log when pr...  26/9/2023      6   \n",
              "2775  HDFS-17209. Correct comments to align with the...  1/10/2023      2   \n",
              "\n",
              "      files                                     critical_files  \\\n",
              "0       352  [Configuration.java, FSDataOutputStream.java, ...   \n",
              "1        37                                 [StringUtils.java]   \n",
              "2         4                                 [StringUtils.java]   \n",
              "3         4             [Configuration.java, StringUtils.java]   \n",
              "4         3                               [Configuration.java]   \n",
              "...     ...                                                ...   \n",
              "2771     12                                 [NodeManager.java]   \n",
              "2772     11                                [FSNamesystem.java]   \n",
              "2773      1                                [FSNamesystem.java]   \n",
              "2774      1                                [BlockManager.java]   \n",
              "2775      1                                    [DataNode.java]   \n",
              "\n",
              "                                         modified_files  \\\n",
              "0     [core-default.xml, HadoopVersionAnnotation.jav...   \n",
              "1     [hadoop-ec2-init-remote.sh, launch-hadoop-slav...   \n",
              "2     [CHANGES.txt, LocalDirAllocator.java, StringUt...   \n",
              "3     [CHANGES.txt, Configuration.java, StringUtils....   \n",
              "4     [CHANGES.txt, Configuration.java, TestConfigur...   \n",
              "...                                                 ...   \n",
              "2771  [YarnConfiguration.java, NodeManager.java, pom...   \n",
              "2772  [pom.xml, PBHelper.java, HdfsServerConstants.j...   \n",
              "2773                                [FSNamesystem.java]   \n",
              "2774                                [BlockManager.java]   \n",
              "2775                                    [DataNode.java]   \n",
              "\n",
              "                                             diff_files  \\\n",
              "0     [{'Configuration.java': '@@ -0,0 +1,1326 @@\n",
              "+/...   \n",
              "1     [{'StringUtils.java': '@@ -88,7 +88,8 @@ publi...   \n",
              "2     [{'StringUtils.java': '@@ -677,4 +677,24 @@ pu...   \n",
              "3     [{'Configuration.java': '@@ -31,6 +31,7 @@\n",
              " im...   \n",
              "4     [{'Configuration.java': '@@ -595,6 +595,30 @@ ...   \n",
              "...                                                 ...   \n",
              "2771  [{'NodeManager.java': '@@ -1074,4 +1074,9 @@ p...   \n",
              "2772  [{'FSNamesystem.java': '@@ -1898,9 +1898,7 @@ ...   \n",
              "2773  [{'FSNamesystem.java': '@@ -6131,15 +6131,20 @...   \n",
              "2774  [{'BlockManager.java': '@@ -4201,6 +4201,12 @@...   \n",
              "2775  [{'DataNode.java': '@@ -2569,7 +2569,7 @@ publ...   \n",
              "\n",
              "                              diff_files_modified_files  \n",
              "0     [@@ -0,0 +1,444 @@\\n+<?xml version=\"1.0\"?>\\n+<...  \n",
              "1     [@@ -17,7 +17,9 @@ MASTER_HOST=%MASTER_HOST% #...  \n",
              "2     [@@ -68,6 +68,9 @@ Trunk (unreleased changes)\\...  \n",
              "3     [@@ -468,6 +468,9 @@ Trunk (unreleased changes...  \n",
              "4     [@@ -480,6 +480,8 @@ Trunk (unreleased changes...  \n",
              "...                                                 ...  \n",
              "2771  [@@ -517,7 +517,7 @@ public static boolean isA...  \n",
              "2772  [@@ -219,6 +219,10 @@ https://maven.apache.org...  \n",
              "2773  [@@ -6131,15 +6131,20 @@ void releaseBackupNod...  \n",
              "2774  [@@ -4201,6 +4201,12 @@ private void chooseExc...  \n",
              "2775  [@@ -2569,7 +2569,7 @@ public void shutdown() ...  \n",
              "\n",
              "[2776 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d53b54c-d0ce-423b-87c9-ed37dd5ba5e8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hash</th>\n",
              "      <th>msg</th>\n",
              "      <th>date</th>\n",
              "      <th>lines</th>\n",
              "      <th>files</th>\n",
              "      <th>critical_files</th>\n",
              "      <th>modified_files</th>\n",
              "      <th>diff_files</th>\n",
              "      <th>diff_files_modified_files</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5128a9a453d64bfe1ed978cf9ffed27985eeef36</td>\n",
              "      <td>HADOOP-4687 Moving src directories on branch\\n...</td>\n",
              "      <td>19/5/2009</td>\n",
              "      <td>67543</td>\n",
              "      <td>352</td>\n",
              "      <td>[Configuration.java, FSDataOutputStream.java, ...</td>\n",
              "      <td>[core-default.xml, HadoopVersionAnnotation.jav...</td>\n",
              "      <td>[{'Configuration.java': '@@ -0,0 +1,1326 @@\n",
              "+/...</td>\n",
              "      <td>[@@ -0,0 +1,444 @@\\n+&lt;?xml version=\"1.0\"?&gt;\\n+&lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bcd64325a11cb0dd5096ffc093d0ffa68c4fcc58</td>\n",
              "      <td>Merged src/core, src/test/core, src/contrib/ec...</td>\n",
              "      <td>15/6/2009</td>\n",
              "      <td>1799</td>\n",
              "      <td>37</td>\n",
              "      <td>[StringUtils.java]</td>\n",
              "      <td>[hadoop-ec2-init-remote.sh, launch-hadoop-slav...</td>\n",
              "      <td>[{'StringUtils.java': '@@ -88,7 +88,8 @@ publi...</td>\n",
              "      <td>[@@ -17,7 +17,9 @@ MASTER_HOST=%MASTER_HOST% #...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b12d765467fd9a4447c473d613d92883fb09c76b</td>\n",
              "      <td>HADOOP-4687. Merge -r 784663:785643 from trunk...</td>\n",
              "      <td>17/6/2009</td>\n",
              "      <td>73</td>\n",
              "      <td>4</td>\n",
              "      <td>[StringUtils.java]</td>\n",
              "      <td>[CHANGES.txt, LocalDirAllocator.java, StringUt...</td>\n",
              "      <td>[{'StringUtils.java': '@@ -677,4 +677,24 @@ pu...</td>\n",
              "      <td>[@@ -68,6 +68,9 @@ Trunk (unreleased changes)\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a13237975d02b3db913b95845d4b8d6d22f2bac7</td>\n",
              "      <td>HADOOP-2366. Support trimmed strings in Config...</td>\n",
              "      <td>30/6/2009</td>\n",
              "      <td>98</td>\n",
              "      <td>4</td>\n",
              "      <td>[Configuration.java, StringUtils.java]</td>\n",
              "      <td>[CHANGES.txt, Configuration.java, StringUtils....</td>\n",
              "      <td>[{'Configuration.java': '@@ -31,6 +31,7 @@\n",
              " im...</td>\n",
              "      <td>[@@ -468,6 +468,9 @@ Trunk (unreleased changes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3200b2ec588dfa5c50f1ec6192ff93ab9187f82d</td>\n",
              "      <td>HADOOP-6161. Add get/setEnum methods to Config...</td>\n",
              "      <td>20/7/2009</td>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>[Configuration.java]</td>\n",
              "      <td>[CHANGES.txt, Configuration.java, TestConfigur...</td>\n",
              "      <td>[{'Configuration.java': '@@ -595,6 +595,30 @@ ...</td>\n",
              "      <td>[@@ -480,6 +480,8 @@ Trunk (unreleased changes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2771</th>\n",
              "      <td>d5334fa76170b99f1ddd6b307482d226da12f1a9</td>\n",
              "      <td>YARN-6537. Running RM tests against the Router...</td>\n",
              "      <td>3/9/2023</td>\n",
              "      <td>816</td>\n",
              "      <td>12</td>\n",
              "      <td>[NodeManager.java]</td>\n",
              "      <td>[YarnConfiguration.java, NodeManager.java, pom...</td>\n",
              "      <td>[{'NodeManager.java': '@@ -1074,4 +1074,9 @@ p...</td>\n",
              "      <td>[@@ -517,7 +517,7 @@ public static boolean isA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2772</th>\n",
              "      <td>4652d22b9195d35eac4d7e02d1f99ebc6a5835c7</td>\n",
              "      <td>HDFS-17178: BootstrapStandby needs to handle R...</td>\n",
              "      <td>12/9/2023</td>\n",
              "      <td>256</td>\n",
              "      <td>11</td>\n",
              "      <td>[FSNamesystem.java]</td>\n",
              "      <td>[pom.xml, PBHelper.java, HdfsServerConstants.j...</td>\n",
              "      <td>[{'FSNamesystem.java': '@@ -1898,9 +1898,7 @@ ...</td>\n",
              "      <td>[@@ -219,6 +219,10 @@ https://maven.apache.org...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2773</th>\n",
              "      <td>ecee022e49269bd6612d32695c6b46a95c6bf11e</td>\n",
              "      <td>HDFS-17197. Show file replication when listing...</td>\n",
              "      <td>25/9/2023</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>[FSNamesystem.java]</td>\n",
              "      <td>[FSNamesystem.java]</td>\n",
              "      <td>[{'FSNamesystem.java': '@@ -6131,15 +6131,20 @...</td>\n",
              "      <td>[@@ -6131,15 +6131,20 @@ void releaseBackupNod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2774</th>\n",
              "      <td>26a5f38250de1ac4978427d74c200a968f9a0b65</td>\n",
              "      <td>HDFS-17204. EC: Reduce unnecessary log when pr...</td>\n",
              "      <td>26/9/2023</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>[BlockManager.java]</td>\n",
              "      <td>[BlockManager.java]</td>\n",
              "      <td>[{'BlockManager.java': '@@ -4201,6 +4201,12 @@...</td>\n",
              "      <td>[@@ -4201,6 +4201,12 @@ private void chooseExc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2775</th>\n",
              "      <td>b87180568b261a617dc9645197615efd4c753786</td>\n",
              "      <td>HDFS-17209. Correct comments to align with the...</td>\n",
              "      <td>1/10/2023</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[DataNode.java]</td>\n",
              "      <td>[DataNode.java]</td>\n",
              "      <td>[{'DataNode.java': '@@ -2569,7 +2569,7 @@ publ...</td>\n",
              "      <td>[@@ -2569,7 +2569,7 @@ public void shutdown() ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2776 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d53b54c-d0ce-423b-87c9-ed37dd5ba5e8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d53b54c-d0ce-423b-87c9-ed37dd5ba5e8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d53b54c-d0ce-423b-87c9-ed37dd5ba5e8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eeacb5e7-65f1-4947-81e5-018c73bae609\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eeacb5e7-65f1-4947-81e5-018c73bae609')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eeacb5e7-65f1-4947-81e5-018c73bae609 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_commits_with_critical_files.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK1eaZ7Cmkot",
        "outputId": "dec07eda-bc81-4269-ab16-5622be1c406f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2776 entries, 0 to 2775\n",
            "Data columns (total 9 columns):\n",
            " #   Column                     Non-Null Count  Dtype \n",
            "---  ------                     --------------  ----- \n",
            " 0   hash                       2776 non-null   object\n",
            " 1   msg                        2776 non-null   object\n",
            " 2   date                       2776 non-null   object\n",
            " 3   lines                      2776 non-null   int64 \n",
            " 4   files                      2776 non-null   int64 \n",
            " 5   critical_files             2776 non-null   object\n",
            " 6   modified_files             2776 non-null   object\n",
            " 7   diff_files                 2776 non-null   object\n",
            " 8   diff_files_modified_files  2776 non-null   object\n",
            "dtypes: int64(2), object(7)\n",
            "memory usage: 195.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_commits_with_critical_files.to_excel('hadoop_commits_with_critical_classes.xlsx', index=False)"
      ],
      "metadata": {
        "id": "EEOLXXjy46HZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Extrai issues de um issue tracker"
      ],
      "metadata": {
        "id": "xEXo9gvEFMOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configura ambiente para acessar Issue Tracker"
      ],
      "metadata": {
        "id": "m3Sj7vZBFZO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Jira connection\n",
        "print('Initialize the Jira connection')\n",
        "jira = JIRA(JIRA_SERVER, basic_auth=(username, password))\n",
        "\n",
        "# Create a JiraUtils instance\n",
        "print('Create a JiraUtils instance')\n",
        "jira_utils = JiraUtils(ISSUE_TRACKER_PROJECT, jira)\n",
        "\n",
        "date1 = (2009, 5, 19)\n",
        "date2 = (2023, 10, 4)\n",
        "distance = 120\n",
        "print('Define date intervals')\n",
        "print(f'From: {str(date1)} to: {date2}, by: {distance} days of distance.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGz4teVIFcig",
        "outputId": "3f6eefd3-117b-48e6-ab2e-f5dfbc539ccc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialize the Jira connection\n",
            "Create a JiraUtils instance\n",
            "Define date intervals\n",
            "From: (2009, 5, 19) to: (2023, 10, 4), by: 120 days of distance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recupera os issues do período dado"
      ],
      "metadata": {
        "id": "22RGuRI7JSjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch issues using date intervals\n",
        "print('Fetch issues using date intervals')\n",
        "block_of_issues = jira_utils.get_list_of_block_issues_by_dates(date1, date2, distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiTmhFaIC5Mg",
        "outputId": "dd9eb897-b553-4bb3-ed30-817b95aaa000"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetch issues using date intervals\n",
            "Aguarde...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   2%|▏         | 1/44 [00:03<02:38,  3.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2009/05/19\" and created<=\"2009/09/15\", qtd issues: 244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   5%|▍         | 2/44 [00:07<02:41,  3.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2009/09/16\" and created<=\"2010/01/13\", qtd issues: 211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   7%|▋         | 3/44 [00:11<02:38,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2010/01/14\" and created<=\"2010/05/13\", qtd issues: 262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:   9%|▉         | 4/44 [00:15<02:29,  3.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2010/05/14\" and created<=\"2010/09/10\", qtd issues: 177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  11%|█▏        | 5/44 [00:17<02:05,  3.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2010/09/11\" and created<=\"2011/01/08\", qtd issues: 133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  14%|█▎        | 6/44 [00:20<01:59,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2011/01/09\" and created<=\"2011/05/08\", qtd issues: 169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  16%|█▌        | 7/44 [00:25<02:17,  3.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2011/05/09\" and created<=\"2011/09/05\", qtd issues: 333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  18%|█▊        | 8/44 [00:30<02:33,  4.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2011/09/06\" and created<=\"2012/01/03\", qtd issues: 327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  20%|██        | 9/44 [00:36<02:44,  4.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2012/01/04\" and created<=\"2012/05/02\", qtd issues: 374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  23%|██▎       | 10/44 [00:43<03:05,  5.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2012/05/03\" and created<=\"2012/08/30\", qtd issues: 377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  25%|██▌       | 11/44 [00:49<03:04,  5.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2012/08/31\" and created<=\"2012/12/28\", qtd issues: 382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  27%|██▋       | 12/44 [00:54<02:52,  5.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2012/12/29\" and created<=\"2013/04/27\", qtd issues: 328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  30%|██▉       | 13/44 [01:00<02:54,  5.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2013/04/28\" and created<=\"2013/08/25\", qtd issues: 361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  32%|███▏      | 14/44 [01:05<02:38,  5.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2013/08/26\" and created<=\"2013/12/23\", qtd issues: 268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  34%|███▍      | 15/44 [01:09<02:29,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2013/12/24\" and created<=\"2014/04/22\", qtd issues: 324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  36%|███▋      | 16/44 [01:16<02:37,  5.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2014/04/23\" and created<=\"2014/08/20\", qtd issues: 427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  39%|███▊      | 17/44 [01:23<02:45,  6.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2014/08/21\" and created<=\"2014/12/18\", qtd issues: 416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  41%|████      | 18/44 [01:30<02:40,  6.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2014/12/19\" and created<=\"2015/04/17\", qtd issues: 381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  43%|████▎     | 19/44 [01:37<02:42,  6.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2015/04/18\" and created<=\"2015/08/15\", qtd issues: 429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  45%|████▌     | 20/44 [01:42<02:25,  6.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2015/08/16\" and created<=\"2015/12/13\", qtd issues: 268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  48%|████▊     | 21/44 [01:49<02:27,  6.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2015/12/14\" and created<=\"2016/04/11\", qtd issues: 360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  50%|█████     | 22/44 [01:57<02:31,  6.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2016/04/12\" and created<=\"2016/08/09\", qtd issues: 445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  52%|█████▏    | 23/44 [02:03<02:20,  6.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2016/08/10\" and created<=\"2016/12/07\", qtd issues: 381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  55%|█████▍    | 24/44 [02:10<02:12,  6.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2016/12/08\" and created<=\"2017/04/06\", qtd issues: 398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  57%|█████▋    | 25/44 [02:19<02:17,  7.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2017/04/07\" and created<=\"2017/08/04\", qtd issues: 434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  59%|█████▉    | 26/44 [02:24<01:59,  6.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2017/08/05\" and created<=\"2017/12/02\", qtd issues: 334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  61%|██████▏   | 27/44 [02:28<01:41,  5.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2017/12/03\" and created<=\"2018/04/01\", qtd issues: 259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  64%|██████▎   | 28/44 [02:33<01:30,  5.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2018/04/02\" and created<=\"2018/07/30\", qtd issues: 273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  66%|██████▌   | 29/44 [02:38<01:20,  5.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2018/07/31\" and created<=\"2018/11/27\", qtd issues: 298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  68%|██████▊   | 30/44 [02:42<01:09,  4.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2018/11/28\" and created<=\"2019/03/27\", qtd issues: 245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  70%|███████   | 31/44 [02:46<01:00,  4.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2019/03/28\" and created<=\"2019/07/25\", qtd issues: 244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  73%|███████▎  | 32/44 [02:50<00:52,  4.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2019/07/26\" and created<=\"2019/11/22\", qtd issues: 255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  75%|███████▌  | 33/44 [02:53<00:44,  4.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2019/11/23\" and created<=\"2020/03/21\", qtd issues: 196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  77%|███████▋  | 34/44 [02:56<00:37,  3.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2020/03/22\" and created<=\"2020/07/19\", qtd issues: 192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  80%|███████▉  | 35/44 [03:00<00:33,  3.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2020/07/20\" and created<=\"2020/11/16\", qtd issues: 218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  82%|████████▏ | 36/44 [03:03<00:29,  3.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2020/11/17\" and created<=\"2021/03/16\", qtd issues: 205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  84%|████████▍ | 37/44 [03:06<00:25,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2021/03/17\" and created<=\"2021/07/14\", qtd issues: 201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  86%|████████▋ | 38/44 [03:12<00:25,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2021/07/15\" and created<=\"2021/11/11\", qtd issues: 195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  89%|████████▊ | 39/44 [03:15<00:19,  3.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2021/11/12\" and created<=\"2022/03/11\", qtd issues: 153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  91%|█████████ | 40/44 [03:18<00:14,  3.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2022/03/12\" and created<=\"2022/07/09\", qtd issues: 170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  93%|█████████▎| 41/44 [03:22<00:10,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2022/07/10\" and created<=\"2022/11/06\", qtd issues: 187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  95%|█████████▌| 42/44 [03:24<00:06,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2022/11/07\" and created<=\"2023/03/06\", qtd issues: 118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis:  98%|█████████▊| 43/44 [03:26<00:02,  2.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2023/03/07\" and created<=\"2023/07/04\", qtd issues: 134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress Message Analysis: 100%|██████████| 44/44 [03:28<00:00,  4.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range: project=HADOOP and created>=\"2023/07/05\" and created<=\"2023/11/01\", qtd issues: 161\n",
            "2023-11-09 13:31:26.415187\n",
            "Tempo da consulta: 0:03:28.809376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the block of issues into a single list\n",
        "print('Concatenate the block of issues into a single list')\n",
        "all_issues = jira_utils.concatenate_block_of_issues(block_of_issues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tOVa-DkJBci",
        "outputId": "3416def3-30c6-4b54-d900-b35be339a598"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenate the block of issues into a single list\n",
            "Total de issues recuperados: 12247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_real_issues = analyze_jira_all_issues(ISSUE_TRACKER_PROJECT, all_issues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7YEk2oZI1lL",
        "outputId": "4b9a2d2e-073b-4515-884d-a86a32504919"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Progress jira all issues analysis: 100%|██████████| 12247/12247 [00:00<00:00, 65052.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converte os issues recuperados para um dataframe"
      ],
      "metadata": {
        "id": "zNu-uRpoKtzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_reall_issues_in_commits_detailed = convert_issues_to_dataframe(all_real_issues)\n",
        "df_all_reall_issues_in_commits_detailed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pg9TMgmwJgNY",
        "outputId": "599ac938-e861-4d05-e9bb-8800ab958674"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          issue_key   issue_type           status priority  \\\n",
              "0      HADOOP-12459  Improvement  Patch Available    Minor   \n",
              "1       HADOOP-6272  Improvement         Resolved    Major   \n",
              "2       HADOOP-6255  New Feature           Closed    Major   \n",
              "3       HADOOP-6254          Bug           Closed    Major   \n",
              "4       HADOOP-6253  New Feature         Resolved    Minor   \n",
              "...             ...          ...              ...      ...   \n",
              "12242  HADOOP-18795     Sub-task         Resolved    Minor   \n",
              "12243  HADOOP-18794  Improvement         Resolved    Major   \n",
              "12244  HADOOP-18793          Bug         Resolved    Minor   \n",
              "12245  HADOOP-18792     Sub-task      In Progress    Major   \n",
              "12246  HADOOP-18791     Sub-task             Open    Major   \n",
              "\n",
              "                                                 summary  \\\n",
              "0                    Implement moveToLocal  HDFS command   \n",
              "1      Incorrect UserName at Solaris because it has n...   \n",
              "2                      Create an rpm integration project   \n",
              "3                  s3n fails with SocketTimeoutException   \n",
              "4                       Add a Ceph FileSystem interface.   \n",
              "...                                                  ...   \n",
              "12242  s3a DelegationToken plugin to expand return ty...   \n",
              "12243  ipc.server.handler.queue.size missing from cor...   \n",
              "12244  S3A StagingCommitter does not clean up staging...   \n",
              "12245  s3a prefetching to use split start/end options...   \n",
              "12246  S3A prefetching: switch to prefetching for cho...   \n",
              "\n",
              "                                             description  \\\n",
              "0      Surprisingly  executing HDFS FsShell command -...   \n",
              "1      Solaris enviroment has no __whoami__ command, ...   \n",
              "2      We should be able to create RPMs for Hadoop re...   \n",
              "3      If a user's map function is CPU intensive and ...   \n",
              "4      The experimental distributed filesystem Ceph d...   \n",
              "...                                                  ...   \n",
              "12242  Change the binding result of s3a DT services b...   \n",
              "12243  This config should be documented. It is necess...   \n",
              "12244  When setting up StagingCommitter and its inter...   \n",
              "12245  the bundled hadoop record readers pass the spl...   \n",
              "12246  before switching to prefetching input stream e...   \n",
              "\n",
              "                                                comments created_date  \\\n",
              "0      {code}\\n$ bin/hadoop fs -moveToLocal\\nmoveToLo...   2009-07-11   \n",
              "1      Is it possible for you to find out the name of...   2009-07-27   \n",
              "2      Hi,\\n\\nI would suggest the other way around: c...   2009-09-11   \n",
              "3      Here is my proposed fix. This patch doesn't in...   2009-09-11   \n",
              "4      I've attached a patch which includes the CephF...   2009-09-11   \n",
              "...                                                  ...          ...   \n",
              "12242  steveloughran opened a new pull request, #5821...   2023-07-08   \n",
              "12243  YuanbenWang opened a new pull request, #5819:\\...   2023-07-08   \n",
              "12244  I'm guessing ${UUID} directory is preserved on...   2023-07-06   \n",
              "12245                                                      2023-07-05   \n",
              "12246  lets get the other big bits in first; my unbuf...   2023-07-05   \n",
              "\n",
              "      resolved_date  \n",
              "0               NaT  \n",
              "1        2014-07-23  \n",
              "2        2011-05-27  \n",
              "3        2009-12-09  \n",
              "4        2014-11-04  \n",
              "...             ...  \n",
              "12242    2023-07-20  \n",
              "12243    2023-07-11  \n",
              "12244    2023-07-08  \n",
              "12245           NaT  \n",
              "12246           NaT  \n",
              "\n",
              "[12247 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-969556e5-644f-48b1-beb3-b3c5ea6ca187\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>status</th>\n",
              "      <th>priority</th>\n",
              "      <th>summary</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>created_date</th>\n",
              "      <th>resolved_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HADOOP-12459</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Patch Available</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Implement moveToLocal  HDFS command</td>\n",
              "      <td>Surprisingly  executing HDFS FsShell command -...</td>\n",
              "      <td>{code}\\n$ bin/hadoop fs -moveToLocal\\nmoveToLo...</td>\n",
              "      <td>2009-07-11</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HADOOP-6272</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Major</td>\n",
              "      <td>Incorrect UserName at Solaris because it has n...</td>\n",
              "      <td>Solaris enviroment has no __whoami__ command, ...</td>\n",
              "      <td>Is it possible for you to find out the name of...</td>\n",
              "      <td>2009-07-27</td>\n",
              "      <td>2014-07-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HADOOP-6255</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Closed</td>\n",
              "      <td>Major</td>\n",
              "      <td>Create an rpm integration project</td>\n",
              "      <td>We should be able to create RPMs for Hadoop re...</td>\n",
              "      <td>Hi,\\n\\nI would suggest the other way around: c...</td>\n",
              "      <td>2009-09-11</td>\n",
              "      <td>2011-05-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HADOOP-6254</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Closed</td>\n",
              "      <td>Major</td>\n",
              "      <td>s3n fails with SocketTimeoutException</td>\n",
              "      <td>If a user's map function is CPU intensive and ...</td>\n",
              "      <td>Here is my proposed fix. This patch doesn't in...</td>\n",
              "      <td>2009-09-11</td>\n",
              "      <td>2009-12-09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HADOOP-6253</td>\n",
              "      <td>New Feature</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Minor</td>\n",
              "      <td>Add a Ceph FileSystem interface.</td>\n",
              "      <td>The experimental distributed filesystem Ceph d...</td>\n",
              "      <td>I've attached a patch which includes the CephF...</td>\n",
              "      <td>2009-09-11</td>\n",
              "      <td>2014-11-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12242</th>\n",
              "      <td>HADOOP-18795</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Minor</td>\n",
              "      <td>s3a DelegationToken plugin to expand return ty...</td>\n",
              "      <td>Change the binding result of s3a DT services b...</td>\n",
              "      <td>steveloughran opened a new pull request, #5821...</td>\n",
              "      <td>2023-07-08</td>\n",
              "      <td>2023-07-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12243</th>\n",
              "      <td>HADOOP-18794</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Major</td>\n",
              "      <td>ipc.server.handler.queue.size missing from cor...</td>\n",
              "      <td>This config should be documented. It is necess...</td>\n",
              "      <td>YuanbenWang opened a new pull request, #5819:\\...</td>\n",
              "      <td>2023-07-08</td>\n",
              "      <td>2023-07-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12244</th>\n",
              "      <td>HADOOP-18793</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Minor</td>\n",
              "      <td>S3A StagingCommitter does not clean up staging...</td>\n",
              "      <td>When setting up StagingCommitter and its inter...</td>\n",
              "      <td>I'm guessing ${UUID} directory is preserved on...</td>\n",
              "      <td>2023-07-06</td>\n",
              "      <td>2023-07-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12245</th>\n",
              "      <td>HADOOP-18792</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>In Progress</td>\n",
              "      <td>Major</td>\n",
              "      <td>s3a prefetching to use split start/end options...</td>\n",
              "      <td>the bundled hadoop record readers pass the spl...</td>\n",
              "      <td></td>\n",
              "      <td>2023-07-05</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12246</th>\n",
              "      <td>HADOOP-18791</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>Open</td>\n",
              "      <td>Major</td>\n",
              "      <td>S3A prefetching: switch to prefetching for cho...</td>\n",
              "      <td>before switching to prefetching input stream e...</td>\n",
              "      <td>lets get the other big bits in first; my unbuf...</td>\n",
              "      <td>2023-07-05</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12247 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-969556e5-644f-48b1-beb3-b3c5ea6ca187')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-969556e5-644f-48b1-beb3-b3c5ea6ca187 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-969556e5-644f-48b1-beb3-b3c5ea6ca187');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3d771a38-3041-4d1a-937c-c30fc545e1d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3d771a38-3041-4d1a-937c-c30fc545e1d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3d771a38-3041-4d1a-937c-c30fc545e1d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colunas = ['issue_key',\t'issue_type', 'status', 'priority', 'summary']\n",
        "#df_all_reall_issues_in_commits_detailed[colunas].to_excel('hadoop_all_issues_in_commits.xlsx', index=False)"
      ],
      "metadata": {
        "id": "vRoqpkZB5SPO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_reall_issues_in_commits_detailed['time_resolution'] = df_all_reall_issues_in_commits_detailed['resolved_date'] - df_all_reall_issues_in_commits_detailed['created_date']"
      ],
      "metadata": {
        "id": "-OkZ5oQsglPd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Gera arquivo .csv hadoop_all_issues_in_commits')\n",
        "colunas = ['issue_key', 'issue_type', 'status', 'summary','created_date', 'resolved_date', 'time_resolution']\n",
        "df_all_reall_issues_in_commits_detailed[colunas].to_csv('hadoop_all_issues_in_commits.csv', index=False)"
      ],
      "metadata": {
        "id": "bFJ71yoFgnY_",
        "outputId": "16d2d68b-06e1-4730-c9a9-7728da8eb265",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gera arquivo .csv hadoop_all_issues_in_commits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recupera Issues com referência nos commits com classes críticas"
      ],
      "metadata": {
        "id": "0AGqE-DqPgBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_issues_in_commits, df_issues_in_commits_with_critical_classes = get_commits_with_critical_files_and_issues_in_this_commits(df_commits_with_critical_files)\n",
        "df_issues_in_commits_with_critical_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "s2Sdfa0pTrGd",
        "outputId": "18141742-e5d4-412b-8a9d-6710033055e4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          issue_key   issue_type    status priority  \\\n",
              "5       HADOOP-6252  Improvement    Closed    Major   \n",
              "13      HADOOP-6243          Bug    Closed  Blocker   \n",
              "21      HADOOP-6233  Improvement    Closed    Major   \n",
              "26      HADOOP-6227          Bug    Closed    Major   \n",
              "63      HADOOP-6184          Bug    Closed    Major   \n",
              "...             ...          ...       ...      ...   \n",
              "11628  HADOOP-18177     Sub-task  Resolved    Major   \n",
              "11630  HADOOP-18175     Sub-task  Resolved    Major   \n",
              "11698  HADOOP-18469  Improvement  Resolved    Major   \n",
              "11786  HADOOP-18379     Sub-task  Resolved    Major   \n",
              "11855  HADOOP-18631     Sub-task  Resolved    Major   \n",
              "\n",
              "                                                 summary  \\\n",
              "5      Provide method to determine if a deprecated ke...   \n",
              "13        NPE in handling deprecated configuration keys.   \n",
              "21     Changes in common to rename the config keys as...   \n",
              "26     Configuration does not lock parameters marked ...   \n",
              "63          Provide a configuration dump in json format.   \n",
              "...                                                  ...   \n",
              "11628  document use and architecture design of prefet...   \n",
              "11630    test failures with prefetching s3a input stream   \n",
              "11698  Add XMLUtils methods to centralise code that c...   \n",
              "11786  rebase feature/HADOOP-18028-s3a-prefetch to trunk   \n",
              "11855        Migrate Async appenders to log4j properties   \n",
              "\n",
              "                                             description  \\\n",
              "5      HADOOP-6105 provided a method to deprecate con...   \n",
              "13     I run TestFileCreation in Eclipse and get a Nu...   \n",
              "21     This jira tracks the code changes required in ...   \n",
              "26     A use-case: Recently, we stumbled on a bug tha...   \n",
              "63                    Configuration dump in json format.   \n",
              "...                                                  ...   \n",
              "11628  Document S3PrefetchingInputStream for users  (...   \n",
              "11630  identify and fix all test regressions from the...   \n",
              "11698  Relates to HDFS-16766\\r\\n\\r\\nThere are other p...   \n",
              "11786  rebase to trunk, fix conflicts and tests, forc...   \n",
              "11855  Before we can upgrade to log4j2, we need to mi...   \n",
              "\n",
              "                                                comments created_date  \\\n",
              "5      Patch:\\n   * Implements new method to see if d...   2009-09-10   \n",
              "13     Running the same test using ant command line w...   2009-09-07   \n",
              "21     The attached patch is created after merging th...   2009-09-02   \n",
              "26     The code in configuration looks like this:\\n\\n...   2009-09-01   \n",
              "63     In order to generate the  dump in standard for...   2009-08-10   \n",
              "...                                                  ...          ...   \n",
              "11628  [~stevel@apache.org] have added an architectur...   2022-03-25   \n",
              "11630  h3. ITestS3AContractUnbuffer\\r\\n\\r\\nfix: getPo...   2022-03-25   \n",
              "11698  pjfanning opened a new pull request, #4940:\\nU...   2022-09-27   \n",
              "11786                                                      2022-07-28   \n",
              "11855  virajjasani opened a new pull request, #5418:\\...   2023-02-15   \n",
              "\n",
              "      resolved_date time_resolution  \n",
              "5        2009-09-11          1 days  \n",
              "13       2009-09-08          1 days  \n",
              "21       2009-09-19         17 days  \n",
              "26       2009-09-01          0 days  \n",
              "63       2009-08-24         14 days  \n",
              "...             ...             ...  \n",
              "11628    2022-04-26         32 days  \n",
              "11630    2022-05-05         41 days  \n",
              "11698    2022-10-07         10 days  \n",
              "11786    2022-08-03          6 days  \n",
              "11855    2023-03-17         30 days  \n",
              "\n",
              "[239 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1fd0f60f-eeeb-4aa5-86fa-1b7c0fb7dc1d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_key</th>\n",
              "      <th>issue_type</th>\n",
              "      <th>status</th>\n",
              "      <th>priority</th>\n",
              "      <th>summary</th>\n",
              "      <th>description</th>\n",
              "      <th>comments</th>\n",
              "      <th>created_date</th>\n",
              "      <th>resolved_date</th>\n",
              "      <th>time_resolution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HADOOP-6252</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Closed</td>\n",
              "      <td>Major</td>\n",
              "      <td>Provide method to determine if a deprecated ke...</td>\n",
              "      <td>HADOOP-6105 provided a method to deprecate con...</td>\n",
              "      <td>Patch:\\n   * Implements new method to see if d...</td>\n",
              "      <td>2009-09-10</td>\n",
              "      <td>2009-09-11</td>\n",
              "      <td>1 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>HADOOP-6243</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Closed</td>\n",
              "      <td>Blocker</td>\n",
              "      <td>NPE in handling deprecated configuration keys.</td>\n",
              "      <td>I run TestFileCreation in Eclipse and get a Nu...</td>\n",
              "      <td>Running the same test using ant command line w...</td>\n",
              "      <td>2009-09-07</td>\n",
              "      <td>2009-09-08</td>\n",
              "      <td>1 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>HADOOP-6233</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Closed</td>\n",
              "      <td>Major</td>\n",
              "      <td>Changes in common to rename the config keys as...</td>\n",
              "      <td>This jira tracks the code changes required in ...</td>\n",
              "      <td>The attached patch is created after merging th...</td>\n",
              "      <td>2009-09-02</td>\n",
              "      <td>2009-09-19</td>\n",
              "      <td>17 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>HADOOP-6227</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Closed</td>\n",
              "      <td>Major</td>\n",
              "      <td>Configuration does not lock parameters marked ...</td>\n",
              "      <td>A use-case: Recently, we stumbled on a bug tha...</td>\n",
              "      <td>The code in configuration looks like this:\\n\\n...</td>\n",
              "      <td>2009-09-01</td>\n",
              "      <td>2009-09-01</td>\n",
              "      <td>0 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>HADOOP-6184</td>\n",
              "      <td>Bug</td>\n",
              "      <td>Closed</td>\n",
              "      <td>Major</td>\n",
              "      <td>Provide a configuration dump in json format.</td>\n",
              "      <td>Configuration dump in json format.</td>\n",
              "      <td>In order to generate the  dump in standard for...</td>\n",
              "      <td>2009-08-10</td>\n",
              "      <td>2009-08-24</td>\n",
              "      <td>14 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11628</th>\n",
              "      <td>HADOOP-18177</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Major</td>\n",
              "      <td>document use and architecture design of prefet...</td>\n",
              "      <td>Document S3PrefetchingInputStream for users  (...</td>\n",
              "      <td>[~stevel@apache.org] have added an architectur...</td>\n",
              "      <td>2022-03-25</td>\n",
              "      <td>2022-04-26</td>\n",
              "      <td>32 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11630</th>\n",
              "      <td>HADOOP-18175</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Major</td>\n",
              "      <td>test failures with prefetching s3a input stream</td>\n",
              "      <td>identify and fix all test regressions from the...</td>\n",
              "      <td>h3. ITestS3AContractUnbuffer\\r\\n\\r\\nfix: getPo...</td>\n",
              "      <td>2022-03-25</td>\n",
              "      <td>2022-05-05</td>\n",
              "      <td>41 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11698</th>\n",
              "      <td>HADOOP-18469</td>\n",
              "      <td>Improvement</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Major</td>\n",
              "      <td>Add XMLUtils methods to centralise code that c...</td>\n",
              "      <td>Relates to HDFS-16766\\r\\n\\r\\nThere are other p...</td>\n",
              "      <td>pjfanning opened a new pull request, #4940:\\nU...</td>\n",
              "      <td>2022-09-27</td>\n",
              "      <td>2022-10-07</td>\n",
              "      <td>10 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11786</th>\n",
              "      <td>HADOOP-18379</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Major</td>\n",
              "      <td>rebase feature/HADOOP-18028-s3a-prefetch to trunk</td>\n",
              "      <td>rebase to trunk, fix conflicts and tests, forc...</td>\n",
              "      <td></td>\n",
              "      <td>2022-07-28</td>\n",
              "      <td>2022-08-03</td>\n",
              "      <td>6 days</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11855</th>\n",
              "      <td>HADOOP-18631</td>\n",
              "      <td>Sub-task</td>\n",
              "      <td>Resolved</td>\n",
              "      <td>Major</td>\n",
              "      <td>Migrate Async appenders to log4j properties</td>\n",
              "      <td>Before we can upgrade to log4j2, we need to mi...</td>\n",
              "      <td>virajjasani opened a new pull request, #5418:\\...</td>\n",
              "      <td>2023-02-15</td>\n",
              "      <td>2023-03-17</td>\n",
              "      <td>30 days</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>239 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fd0f60f-eeeb-4aa5-86fa-1b7c0fb7dc1d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1fd0f60f-eeeb-4aa5-86fa-1b7c0fb7dc1d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1fd0f60f-eeeb-4aa5-86fa-1b7c0fb7dc1d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2484e93-af12-4ae2-9322-94be8947058f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2484e93-af12-4ae2-9322-94be8947058f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2484e93-af12-4ae2-9322-94be8947058f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues_in_commits_with_critical_classes.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGu-1BkUmbEx",
        "outputId": "c6c66691-7ea0-475e-ac1f-a76b310ec043"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 239 entries, 5 to 11855\n",
            "Data columns (total 10 columns):\n",
            " #   Column           Non-Null Count  Dtype          \n",
            "---  ------           --------------  -----          \n",
            " 0   issue_key        239 non-null    object         \n",
            " 1   issue_type       239 non-null    object         \n",
            " 2   status           239 non-null    object         \n",
            " 3   priority         239 non-null    object         \n",
            " 4   summary          239 non-null    object         \n",
            " 5   description      239 non-null    object         \n",
            " 6   comments         239 non-null    object         \n",
            " 7   created_date     239 non-null    datetime64[ns] \n",
            " 8   resolved_date    236 non-null    datetime64[ns] \n",
            " 9   time_resolution  236 non-null    timedelta64[ns]\n",
            "dtypes: datetime64[ns](2), object(7), timedelta64[ns](1)\n",
            "memory usage: 20.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues_in_commits_with_critical_classes['time_resolution'] = df_issues_in_commits_with_critical_classes['resolved_date'] - df_issues_in_commits_with_critical_classes['created_date']"
      ],
      "metadata": {
        "id": "APX8lgSbhsjE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_issues_in_commits_with_critical_classes[colunas].to_excel('hadoop_issues_in_commits_with_critical_classes.xlsx', index=False)"
      ],
      "metadata": {
        "id": "DwlH9PvehwgG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Gera arquivo .csv kafka_issues_in_commits_arquivos_criticos')\n",
        "df_issues_in_commits_with_critical_classes[colunas].to_csv('hadoop_issues_in_commits_with_critical_classes.csv', index=False)"
      ],
      "metadata": {
        "id": "eZHhxypY5qPm",
        "outputId": "29a3bf8f-ba11-4929-8d69-ee710c564f7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gera arquivo .csv kafka_issues_in_commits_arquivos_criticos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seleciona amostra dos Issues"
      ],
      "metadata": {
        "id": "tSu8U-M7XJdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_level = 0.98\n",
        "margin_of_error = 0.05\n",
        "population_proportion = 0.8\n",
        "population_size = len(df_issues_in_commits_with_critical_classes)\n",
        "\n",
        "sample_size = calculate_sample_size(confidence_level, margin_of_error, population_proportion, population_size)\n",
        "print(f'População de issues: {population_size}')\n",
        "print(f\"Sample size para inspeção: {sample_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsrUjy68VaC_",
        "outputId": "052868ac-2731-4d74-d362-9ead7a0979f6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "População de issues: 239\n",
            "Sample size para inspeção: 141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_issues = select_issues_to_inspection(sample_size, df_issues_in_commits_with_critical_classes, my_date='09/11/2023')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBfy--OYYLWS",
        "outputId": "7259fd78-8807-4a13-d544-a105d89bc85d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "239 para inspeção manual\n",
            "Relação de Issues salvos em 09/11/2023 para inspeção.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_files_issues_to_inspection(sample_issues, df_issues_in_commits_with_critical_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5-WsdGzfQpH",
        "outputId": "9df833ee-d024-42bd-e394-ea1a3703434d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing issues:  27%|██▋       | 65/239 [00:00<00:00, 318.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File my_issues/HADOOP-6252 created with success!\n",
            "File my_issues/HADOOP-6252 created with success!\n",
            "File my_issues/HADOOP-6184 created with success!\n",
            "File my_issues/HADOOP-6184 created with success!\n",
            "File my_issues/HADOOP-6165 created with success!\n",
            "File my_issues/HADOOP-6165 created with success!\n",
            "File my_issues/HADOOP-6161 created with success!\n",
            "File my_issues/HADOOP-6161 created with success!\n",
            "File my_issues/HADOOP-6161 created with success!\n",
            "File my_issues/HADOOP-6103 created with success!\n",
            "File my_issues/HADOOP-6103 created with success!\n",
            "File my_issues/HADOOP-6471 created with success!\n",
            "File my_issues/HADOOP-6443 created with success!\n",
            "File my_issues/HADOOP-6443 created with success!\n",
            "File my_issues/HADOOP-6420 created with success!\n",
            "File my_issues/HADOOP-6346 created with success!\n",
            "File my_issues/HADOOP-6323 created with success!\n",
            "File my_issues/HADOOP-6323 created with success!\n",
            "File my_issues/HADOOP-6323 created with success!\n",
            "File my_issues/HADOOP-6313 created with success!\n",
            "File my_issues/HADOOP-6312 created with success!\n",
            "File my_issues/HADOOP-6312 created with success!\n",
            "File my_issues/HADOOP-6298 created with success!\n",
            "File my_issues/HADOOP-6298 created with success!\n",
            "File my_issues/HADOOP-6269 created with success!\n",
            "File my_issues/HADOOP-6698 created with success!\n",
            "File my_issues/HADOOP-6671 created with success!\n",
            "File my_issues/HADOOP-6671 created with success!\n",
            "File my_issues/HADOOP-6623 created with success!\n",
            "File my_issues/HADOOP-6578 created with success!\n",
            "File my_issues/HADOOP-6578 created with success!\n",
            "File my_issues/HADOOP-6521 created with success!\n",
            "File my_issues/HADOOP-6521 created with success!\n",
            "File my_issues/HADOOP-6521 created with success!\n",
            "File my_issues/HADOOP-6502 created with success!\n",
            "File my_issues/HADOOP-6502 created with success!\n",
            "File my_issues/HADOOP-6502 created with success!\n",
            "File my_issues/HADOOP-6802 created with success!\n",
            "File my_issues/HADOOP-6791 created with success!\n",
            "File my_issues/HADOOP-7082 created with success!\n",
            "File my_issues/HADOOP-7046 created with success!\n",
            "File my_issues/HADOOP-7046 created with success!\n",
            "File my_issues/HADOOP-7046 created with success!\n",
            "File my_issues/HADOOP-7046 created with success!\n",
            "File my_issues/HADOOP-7046 created with success!\n",
            "File my_issues/HADOOP-7001 created with success!\n",
            "File my_issues/HADOOP-6964 created with success!\n",
            "File my_issues/HADOOP-6964 created with success!\n",
            "File my_issues/HADOOP-7151 created with success!\n",
            "File my_issues/HADOOP-7145 created with success!\n",
            "File my_issues/HADOOP-7118 created with success!\n",
            "File my_issues/HADOOP-7106 created with success!\n",
            "File my_issues/HADOOP-7547 created with success!\n",
            "File my_issues/HADOOP-7547 created with success!\n",
            "File my_issues/HADOOP-7524 created with success!\n",
            "File my_issues/HADOOP-7451 created with success!\n",
            "File my_issues/HADOOP-7451 created with success!\n",
            "File my_issues/HADOOP-7287 created with success!\n",
            "File my_issues/HADOOP-7910 created with success!\n",
            "File my_issues/HADOOP-7851 created with success!\n",
            "File my_issues/HADOOP-7851 created with success!\n",
            "File my_issues/HADOOP-7851 created with success!\n",
            "File my_issues/HADOOP-7664 created with success!\n",
            "File my_issues/HADOOP-7664 created with success!\n",
            "File my_issues/HADOOP-8335 created with success!\n",
            "File my_issues/HADOOP-8325 created with success!\n",
            "File my_issues/HADOOP-8325 created with success!\n",
            "File my_issues/HADOOP-8297 created with success!\n",
            "File my_issues/HADOOP-8286 created with success!\n",
            "File my_issues/HADOOP-8286 created with success!\n",
            "File my_issues/HADOOP-8240 created with success!\n",
            "File my_issues/HADOOP-8227 created with success!\n",
            "File my_issues/HADOOP-8172 created with success!\n",
            "File my_issues/HADOOP-8172 created with success!\n",
            "File my_issues/HADOOP-8172 created with success!\n",
            "File my_issues/HADOOP-8167 created with success!\n",
            "File my_issues/HADOOP-8167 created with success!\n",
            "File my_issues/HADOOP-8157 created with success!\n",
            "File my_issues/HADOOP-8124 created with success!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing issues:  55%|█████▍    | 131/239 [00:00<00:00, 322.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File my_issues/HADOOP-7993 created with success!\n",
            "File my_issues/HADOOP-7993 created with success!\n",
            "File my_issues/HADOOP-8632 created with success!\n",
            "File my_issues/HADOOP-8608 created with success!\n",
            "File my_issues/HADOOP-8573 created with success!\n",
            "File my_issues/HADOOP-8525 created with success!\n",
            "File my_issues/HADOOP-8525 created with success!\n",
            "File my_issues/HADOOP-8524 created with success!\n",
            "File my_issues/HADOOP-8524 created with success!\n",
            "File my_issues/HADOOP-8524 created with success!\n",
            "File my_issues/HADOOP-8524 created with success!\n",
            "File my_issues/HADOOP-8524 created with success!\n",
            "File my_issues/HADOOP-8362 created with success!\n",
            "File my_issues/HADOOP-8362 created with success!\n",
            "File my_issues/HADOOP-8359 created with success!\n",
            "File my_issues/HADOOP-8349 created with success!\n",
            "File my_issues/HADOOP-8952 created with success!\n",
            "File my_issues/HADOOP-8821 created with success!\n",
            "File my_issues/HADOOP-8780 created with success!\n",
            "File my_issues/HADOOP-8780 created with success!\n",
            "File my_issues/HADOOP-8780 created with success!\n",
            "File my_issues/HADOOP-8780 created with success!\n",
            "File my_issues/HADOOP-9361 created with success!\n",
            "File my_issues/HADOOP-9323 created with success!\n",
            "File my_issues/HADOOP-9318 created with success!\n",
            "File my_issues/HADOOP-9252 created with success!\n",
            "File my_issues/HADOOP-9784 created with success!\n",
            "File my_issues/HADOOP-9784 created with success!\n",
            "File my_issues/HADOOP-9756 created with success!\n",
            "File my_issues/HADOOP-9756 created with success!\n",
            "File my_issues/HADOOP-9756 created with success!\n",
            "File my_issues/HADOOP-9688 created with success!\n",
            "File my_issues/HADOOP-9688 created with success!\n",
            "File my_issues/HADOOP-9686 created with success!\n",
            "File my_issues/HADOOP-9686 created with success!\n",
            "File my_issues/HADOOP-9686 created with success!\n",
            "File my_issues/HADOOP-9649 created with success!\n",
            "File my_issues/HADOOP-9618 created with success!\n",
            "File my_issues/HADOOP-9604 created with success!\n",
            "File my_issues/HADOOP-9582 created with success!\n",
            "File my_issues/HADOOP-9582 created with success!\n",
            "File my_issues/HADOOP-10178 created with success!\n",
            "File my_issues/HADOOP-10178 created with success!\n",
            "File my_issues/HADOOP-10075 created with success!\n",
            "File my_issues/HADOOP-10482 created with success!\n",
            "File my_issues/HADOOP-10402 created with success!\n",
            "File my_issues/HADOOP-10345 created with success!\n",
            "File my_issues/HADOOP-10248 created with success!\n",
            "File my_issues/HADOOP-10248 created with success!\n",
            "File my_issues/HADOOP-10208 created with success!\n",
            "File my_issues/HADOOP-11399 created with success!\n",
            "File my_issues/HADOOP-10873 created with success!\n",
            "File my_issues/HADOOP-10682 created with success!\n",
            "File my_issues/HADOOP-10625 created with success!\n",
            "File my_issues/HADOOP-10625 created with success!\n",
            "File my_issues/HADOOP-10607 created with success!\n",
            "File my_issues/HADOOP-11416 created with success!\n",
            "File my_issues/HADOOP-11389 created with success!\n",
            "File my_issues/HADOOP-11274 created with success!\n",
            "File my_issues/HADOOP-11209 created with success!\n",
            "File my_issues/HADOOP-11209 created with success!\n",
            "File my_issues/HADOOP-11741 created with success!\n",
            "File my_issues/HADOOP-11741 created with success!\n",
            "File my_issues/HADOOP-11627 created with success!\n",
            "File my_issues/HADOOP-11506 created with success!\n",
            "File my_issues/HADOOP-11506 created with success!\n",
            "File my_issues/HADOOP-12321 created with success!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing issues:  82%|████████▏ | 197/239 [00:00<00:00, 314.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File my_issues/HADOOP-12317 created with success!\n",
            "File my_issues/HADOOP-12317 created with success!\n",
            "File my_issues/HADOOP-12317 created with success!\n",
            "File my_issues/HADOOP-12097 created with success!\n",
            "File my_issues/HADOOP-12097 created with success!\n",
            "File my_issues/HADOOP-12097 created with success!\n",
            "File my_issues/HADOOP-11901 created with success!\n",
            "File my_issues/HADOOP-11901 created with success!\n",
            "File my_issues/HADOOP-11852 created with success!\n",
            "File my_issues/HADOOP-12457 created with success!\n",
            "File my_issues/HADOOP-12457 created with success!\n",
            "File my_issues/HADOOP-12457 created with success!\n",
            "File my_issues/HADOOP-12457 created with success!\n",
            "File my_issues/HADOOP-12437 created with success!\n",
            "File my_issues/HADOOP-13770 created with success!\n",
            "File my_issues/HADOOP-12916 created with success!\n",
            "File my_issues/HADOOP-12862 created with success!\n",
            "File my_issues/HADOOP-12862 created with success!\n",
            "File my_issues/HADOOP-12639 created with success!\n",
            "File my_issues/HADOOP-13442 created with success!\n",
            "File my_issues/HADOOP-13441 created with success!\n",
            "File my_issues/HADOOP-13441 created with success!\n",
            "File my_issues/HADOOP-13327 created with success!\n",
            "File my_issues/HADOOP-13706 created with success!\n",
            "File my_issues/HADOOP-13628 created with success!\n",
            "File my_issues/HADOOP-13500 created with success!\n",
            "File my_issues/HADOOP-14260 created with success!\n",
            "File my_issues/HADOOP-14260 created with success!\n",
            "File my_issues/HADOOP-14251 created with success!\n",
            "File my_issues/HADOOP-14251 created with success!\n",
            "File my_issues/HADOOP-14251 created with success!\n",
            "File my_issues/HADOOP-14104 created with success!\n",
            "File my_issues/HADOOP-14038 created with success!\n",
            "File my_issues/HADOOP-14727 created with success!\n",
            "File my_issues/HADOOP-14727 created with success!\n",
            "File my_issues/HADOOP-14701 created with success!\n",
            "File my_issues/HADOOP-14503 created with success!\n",
            "File my_issues/HADOOP-14341 created with success!\n",
            "File my_issues/HADOOP-14938 created with success!\n",
            "File my_issues/HADOOP-14938 created with success!\n",
            "File my_issues/HADOOP-15331 created with success!\n",
            "File my_issues/HADOOP-15554 created with success!\n",
            "File my_issues/HADOOP-15357 created with success!\n",
            "File my_issues/HADOOP-15755 created with success!\n",
            "File my_issues/HADOOP-15755 created with success!\n",
            "File my_issues/HADOOP-15708 created with success!\n",
            "File my_issues/HADOOP-15708 created with success!\n",
            "File my_issues/HADOOP-15708 created with success!\n",
            "File my_issues/HADOOP-16084 created with success!\n",
            "File my_issues/HADOOP-16084 created with success!\n",
            "File my_issues/HADOOP-16084 created with success!\n",
            "File my_issues/HADOOP-16282 created with success!\n",
            "File my_issues/HADOOP-16282 created with success!\n",
            "File my_issues/HADOOP-16282 created with success!\n",
            "File my_issues/HADOOP-16265 created with success!\n",
            "File my_issues/HADOOP-16218 created with success!\n",
            "File my_issues/HADOOP-16218 created with success!\n",
            "File my_issues/HADOOP-16218 created with success!\n",
            "File my_issues/HADOOP-16654 created with success!\n",
            "File my_issues/HADOOP-16654 created with success!\n",
            "File my_issues/HADOOP-16596 created with success!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing issues: 100%|██████████| 239/239 [00:00<00:00, 318.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File my_issues/HADOOP-16906 created with success!\n",
            "File my_issues/HADOOP-16830 created with success!\n",
            "File my_issues/HADOOP-17123 created with success!\n",
            "File my_issues/HADOOP-17088 created with success!\n",
            "File my_issues/HADOOP-17079 created with success!\n",
            "File my_issues/HADOOP-16951 created with success!\n",
            "File my_issues/HADOOP-16951 created with success!\n",
            "File my_issues/HADOOP-16951 created with success!\n",
            "File my_issues/HADOOP-17358 created with success!\n",
            "File my_issues/HADOOP-17358 created with success!\n",
            "File my_issues/HADOOP-17288 created with success!\n",
            "File my_issues/HADOOP-17424 created with success!\n",
            "File my_issues/HADOOP-17613 created with success!\n",
            "File my_issues/HADOOP-17963 created with success!\n",
            "File my_issues/HADOOP-17963 created with success!\n",
            "File my_issues/HADOOP-17959 created with success!\n",
            "File my_issues/HADOOP-17959 created with success!\n",
            "File my_issues/HADOOP-17957 created with success!\n",
            "File my_issues/HADOOP-18028 created with success!\n",
            "File my_issues/HADOOP-18028 created with success!\n",
            "File my_issues/HADOOP-18014 created with success!\n",
            "File my_issues/HADOOP-18014 created with success!\n",
            "File my_issues/HADOOP-18318 created with success!\n",
            "File my_issues/HADOOP-18266 created with success!\n",
            "File my_issues/HADOOP-18229 created with success!\n",
            "File my_issues/HADOOP-18229 created with success!\n",
            "File my_issues/HADOOP-18190 created with success!\n",
            "File my_issues/HADOOP-18180 created with success!\n",
            "File my_issues/HADOOP-18180 created with success!\n",
            "File my_issues/HADOOP-18180 created with success!\n",
            "File my_issues/HADOOP-18180 created with success!\n",
            "File my_issues/HADOOP-18379 created with success!\n",
            "Foram criados 239 arquivos para inspeção\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r my_issues.zip my_issues"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJfW2I7Jm_8T",
        "outputId": "42c93bb6-ed4d-403a-8fa2-6aaebe5f00d0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: my_issues/ (stored 0%)\n",
            "  adding: my_issues/HADOOP-10402 (deflated 73%)\n",
            "  adding: my_issues/HADOOP-6471 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-6252 (deflated 62%)\n",
            "  adding: my_issues/HADOOP-10682 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-6323 (deflated 59%)\n",
            "  adding: my_issues/HADOOP-7524 (deflated 57%)\n",
            "  adding: my_issues/HADOOP-9582 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-16596 (deflated 70%)\n",
            "  adding: my_issues/HADOOP-9688 (deflated 62%)\n",
            "  adding: my_issues/HADOOP-13706 (deflated 78%)\n",
            "  adding: my_issues/HADOOP-17088 (deflated 55%)\n",
            "  adding: my_issues/HADOOP-6165 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-6269 (deflated 62%)\n",
            "  adding: my_issues/HADOOP-10482 (deflated 68%)\n",
            "  adding: my_issues/HADOOP-8359 (deflated 75%)\n",
            "  adding: my_issues/HADOOP-12916 (deflated 75%)\n",
            "  adding: my_issues/HADOOP-18229 (deflated 76%)\n",
            "  adding: my_issues/HADOOP-10208 (deflated 73%)\n",
            "  adding: my_issues/HADOOP-6184 (deflated 57%)\n",
            "  adding: my_issues/HADOOP-6312 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-8573 (deflated 64%)\n",
            "  adding: my_issues/HADOOP-7287 (deflated 66%)\n",
            "  adding: my_issues/HADOOP-16951 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-8362 (deflated 68%)\n",
            "  adding: my_issues/HADOOP-11852 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-18190 (deflated 52%)\n",
            "  adding: my_issues/HADOOP-16654 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-10625 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-15554 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-7910 (deflated 57%)\n",
            "  adding: my_issues/HADOOP-6313 (deflated 66%)\n",
            "  adding: my_issues/HADOOP-6443 (deflated 70%)\n",
            "  adding: my_issues/HADOOP-17358 (deflated 52%)\n",
            "  adding: my_issues/HADOOP-16906 (deflated 50%)\n",
            "  adding: my_issues/HADOOP-17424 (deflated 49%)\n",
            "  adding: my_issues/HADOOP-10607 (deflated 55%)\n",
            "  adding: my_issues/HADOOP-6964 (deflated 68%)\n",
            "  adding: my_issues/HADOOP-8821 (deflated 73%)\n",
            "  adding: my_issues/HADOOP-8952 (deflated 43%)\n",
            "  adding: my_issues/HADOOP-6671 (deflated 53%)\n",
            "  adding: my_issues/HADOOP-7145 (deflated 55%)\n",
            "  adding: my_issues/HADOOP-8349 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-17123 (deflated 31%)\n",
            "  adding: my_issues/HADOOP-18266 (deflated 20%)\n",
            "  adding: my_issues/HADOOP-11741 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-18318 (deflated 33%)\n",
            "  adding: my_issues/HADOOP-11399 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-7106 (deflated 56%)\n",
            "  adding: my_issues/HADOOP-8286 (deflated 72%)\n",
            "  adding: my_issues/HADOOP-17613 (deflated 79%)\n",
            "  adding: my_issues/HADOOP-9686 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-6623 (deflated 72%)\n",
            "  adding: my_issues/HADOOP-8240 (deflated 54%)\n",
            "  adding: my_issues/HADOOP-14260 (deflated 72%)\n",
            "  adding: my_issues/HADOOP-10248 (deflated 73%)\n",
            "  adding: my_issues/HADOOP-15331 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-6791 (deflated 64%)\n",
            "  adding: my_issues/HADOOP-11209 (deflated 68%)\n",
            "  adding: my_issues/HADOOP-9649 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-7001 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-13628 (deflated 69%)\n",
            "  adding: my_issues/HADOOP-14503 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-6346 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-8227 (deflated 57%)\n",
            "  adding: my_issues/HADOOP-14938 (deflated 72%)\n",
            "  adding: my_issues/HADOOP-14038 (deflated 78%)\n",
            "  adding: my_issues/HADOOP-7451 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-7082 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-9323 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-13442 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-14251 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-14701 (deflated 72%)\n",
            "  adding: my_issues/HADOOP-15755 (deflated 75%)\n",
            "  adding: my_issues/HADOOP-16084 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-11274 (deflated 69%)\n",
            "  adding: my_issues/HADOOP-10075 (deflated 55%)\n",
            "  adding: my_issues/HADOOP-13327 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-16265 (deflated 68%)\n",
            "  adding: my_issues/HADOOP-13441 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-14727 (deflated 63%)\n",
            "  adding: my_issues/HADOOP-9318 (deflated 62%)\n",
            "  adding: my_issues/HADOOP-17957 (deflated 21%)\n",
            "  adding: my_issues/HADOOP-18379 (deflated 21%)\n",
            "  adding: my_issues/HADOOP-8335 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-7664 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-13770 (deflated 76%)\n",
            "  adding: my_issues/HADOOP-6502 (deflated 58%)\n",
            "  adding: my_issues/HADOOP-8325 (deflated 65%)\n",
            "  adding: my_issues/HADOOP-12321 (deflated 58%)\n",
            "  adding: my_issues/HADOOP-17288 (deflated 54%)\n",
            "  adding: my_issues/HADOOP-18014 (deflated 20%)\n",
            "  adding: my_issues/HADOOP-9618 (deflated 56%)\n",
            "  adding: my_issues/HADOOP-16282 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-6578 (deflated 68%)\n",
            "  adding: my_issues/HADOOP-9604 (deflated 69%)\n",
            "  adding: my_issues/HADOOP-8525 (deflated 65%)\n",
            "  adding: my_issues/HADOOP-12862 (deflated 61%)\n",
            "  adding: my_issues/HADOOP-10873 (deflated 77%)\n",
            "  adding: my_issues/HADOOP-9252 (deflated 70%)\n",
            "  adding: my_issues/HADOOP-7046 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-6161 (deflated 58%)\n",
            "  adding: my_issues/HADOOP-14341 (deflated 73%)\n",
            "  adding: my_issues/HADOOP-7118 (deflated 64%)\n",
            "  adding: my_issues/HADOOP-8172 (deflated 66%)\n",
            "  adding: my_issues/HADOOP-8608 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-9784 (deflated 68%)\n",
            "  adding: my_issues/HADOOP-7851 (deflated 69%)\n",
            "  adding: my_issues/HADOOP-15357 (deflated 70%)\n",
            "  adding: my_issues/HADOOP-11389 (deflated 66%)\n",
            "  adding: my_issues/HADOOP-18028 (deflated 50%)\n",
            "  adding: my_issues/HADOOP-12317 (deflated 58%)\n",
            "  adding: my_issues/HADOOP-10345 (deflated 77%)\n",
            "  adding: my_issues/HADOOP-11416 (deflated 76%)\n",
            "  adding: my_issues/HADOOP-6698 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-8780 (deflated 60%)\n",
            "  adding: my_issues/HADOOP-17959 (deflated 28%)\n",
            "  adding: my_issues/HADOOP-9361 (deflated 58%)\n",
            "  adding: my_issues/HADOOP-6802 (deflated 74%)\n",
            "  adding: my_issues/HADOOP-15708 (deflated 57%)\n",
            "  adding: my_issues/HADOOP-16830 (deflated 51%)\n",
            "  adding: my_issues/HADOOP-12639 (deflated 81%)\n",
            "  adding: my_issues/HADOOP-7993 (deflated 66%)\n",
            "  adding: my_issues/HADOOP-6103 (deflated 66%)\n",
            "  adding: my_issues/HADOOP-18180 (deflated 51%)\n",
            "  adding: my_issues/HADOOP-7151 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-6420 (deflated 71%)\n",
            "  adding: my_issues/HADOOP-12457 (deflated 79%)\n",
            "  adding: my_issues/HADOOP-11506 (deflated 53%)\n",
            "  adding: my_issues/HADOOP-16218 (deflated 65%)\n",
            "  adding: my_issues/HADOOP-17963 (deflated 22%)\n",
            "  adding: my_issues/HADOOP-8167 (deflated 65%)\n",
            "  adding: my_issues/HADOOP-14104 (deflated 63%)\n",
            "  adding: my_issues/HADOOP-6298 (deflated 55%)\n",
            "  adding: my_issues/HADOOP-9756 (deflated 70%)\n",
            "  adding: my_issues/HADOOP-8632 (deflated 56%)\n",
            "  adding: my_issues/HADOOP-8124 (deflated 73%)\n",
            "  adding: my_issues/HADOOP-12097 (deflated 79%)\n",
            "  adding: my_issues/HADOOP-13500 (deflated 55%)\n",
            "  adding: my_issues/HADOOP-6521 (deflated 62%)\n",
            "  adding: my_issues/HADOOP-10178 (deflated 72%)\n",
            "  adding: my_issues/HADOOP-11627 (deflated 61%)\n",
            "  adding: my_issues/HADOOP-8157 (deflated 75%)\n",
            "  adding: my_issues/HADOOP-11901 (deflated 56%)\n",
            "  adding: my_issues/HADOOP-12437 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-8524 (deflated 64%)\n",
            "  adding: my_issues/HADOOP-8297 (deflated 66%)\n",
            "  adding: my_issues/HADOOP-7547 (deflated 67%)\n",
            "  adding: my_issues/HADOOP-17079 (deflated 74%)\n"
          ]
        }
      ]
    }
  ]
}